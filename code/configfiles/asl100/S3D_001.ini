[training]
batch_size = 8
update_per_step = 4
max_epoch = 200
early_stopping = {'metric' : ('val','loss'), 'mode': 'min', 'patience': 50, 'min_delta': 0.01}

[optimizer]
eps = 1e-3
backbone_init_lr = 1e-5
backbone_weight_decay = 1e-4
classifier_init_lr = 1e-3
classifier_weight_decay = 1e-7

[model_params]
drop_p = 0.5

[scheduler] 
tmax = 100
eta_min = 1e-5

[data]
num_frames = 16
frame_size = 224


; based on S3D 000
; the run did not perform amazingly well. putting the classifier weight decay to 1e-7
; also increasing the patience to 50 epochs
; also increasing the batch size to 8 (with grad accumulation of 4)
; this configuration uses ~ 5.599Gi/11.000Gi of GPU memory

; as a preliminary result, this has sigificantly sped up training
