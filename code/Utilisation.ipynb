{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "246f1c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/miniconda3/envs/wlasl/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/home/luke/miniconda3/envs/wlasl/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from models import get_model, avail_models, norm_vals\n",
    "from video_dataset import get_data_loader, get_wlasl_info\n",
    "from configs import get_avail_splits\n",
    "import torch\n",
    "import gc\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.profiler import profile, ProfilerActivity, record_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "653857b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee2e38e",
   "metadata": {},
   "source": [
    "### Available models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48259c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3D, R3D_18, R(2+1)D_18, Swin3D_T, Swin3D_S, Swin3D_B, MViTv2_S, MViTv1_B\n"
     ]
    }
   ],
   "source": [
    "avail_m = avail_models()\n",
    "print(', '.join(avail_m))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03246d1c",
   "metadata": {},
   "source": [
    "### Available splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d8dc90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asl100, asl300\n"
     ]
    }
   ],
   "source": [
    "avail_sp = get_avail_splits()\n",
    "print(', '.join(avail_sp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8174d2b9",
   "metadata": {},
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "320e92d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numclasses:  100\n"
     ]
    }
   ],
   "source": [
    "# fs = 224 #frame size\n",
    "# nf = 32 #num frames\n",
    "# bs = 8 #batch size\n",
    "fs = 224 \n",
    "nf = 16\n",
    "bs = 2\n",
    "\n",
    "\n",
    "nvals = norm_vals(avail_m[0]) #normalisation won't make a difference in this case, but S3D\n",
    "\n",
    "wlasl_info = get_wlasl_info(avail_sp[0], 'test') #asl100\n",
    "\n",
    "testloader, ncls, _, _ = get_data_loader(\n",
    "    mean=nvals['mean'],\n",
    "    std=nvals['std'],\n",
    "    frame_size=fs,\n",
    "    num_frames=nf,\n",
    "    set_info=wlasl_info,\n",
    "    batch_size=bs\n",
    ")\n",
    "\n",
    "print(\"Numclasses: \", ncls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a6b2a3",
   "metadata": {},
   "source": [
    "### Get a single video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a69e39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 16, 224, 224])\n",
      "vid0 device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "dicty = next(iter(testloader))\n",
    "vid0, target = dicty[\"frames\"], dicty[\"label_num\"]\n",
    "vid0 = vid0.to(device)  # Fix: reassign the result\n",
    "target = target.to(device)\n",
    "print(vid0.shape)\n",
    "print(f\"vid0 device: {vid0.device}\")  # Verify it's on CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e4272e",
   "metadata": {},
   "source": [
    "## Get models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2006df4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/miniconda3/envs/wlasl/lib/python3.10/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "nc = 100 #num classes\n",
    "dropout = 0.0 #no dropout\n",
    "all_models = []\n",
    "for arch in avail_m:\n",
    "    model = get_model(arch, nc, dropout)\n",
    "    all_models.append((arch, model)) #tuple: arch, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b457f9",
   "metadata": {},
   "source": [
    "## From PyTorch Docs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a1c4a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_it(model, inputs, title):\n",
    "    activities = [ProfilerActivity.CPU]\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "        activities += [ProfilerActivity.CUDA]\n",
    "    elif torch.xpu.is_available():\n",
    "        device = \"xpu\"\n",
    "        activities += [ProfilerActivity.XPU]\n",
    "    else:\n",
    "        print(\n",
    "            \"Neither CUDA nor XPU devices are available to demonstrate profiling on acceleration devices\"\n",
    "        )\n",
    "        import sys\n",
    "\n",
    "        sys.exit(0)\n",
    "\n",
    "    sort_by_keyword = device + \"_time_total\"\n",
    "\n",
    "    model = model.to(device)\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    with profile(activities=activities, record_shapes=True, profile_memory=True) as prof:\n",
    "        with record_function(f\"{title}_inference\"):\n",
    "            model(inputs)\n",
    "\n",
    "    print(prof.key_averages().table(sort_by=sort_by_keyword, row_limit=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4fb1ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for arch, model in all_models:\n",
    "#     profile_it(model, vid0, arch)\n",
    "#     print(\"\\n\"*2, \"-\"*(250-22), \"\\n\"*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28ea9f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "print(len('----------------------'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aee3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline GPU Memory: 188 MiB\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Model: S3D\n",
      "================================================================================\n",
      "\n",
      "Inference Time: 26.47 ms\n",
      "Throughput: 75.56 samples/sec\n",
      "\n",
      "GPU Memory Usage (via nvidia-smi):\n",
      "  Before model load: 188 MiB\n",
      "  After model load:  224 MiB  (+36 MiB)\n",
      "  Before inference:  444 MiB\n",
      "  Peak during run:   444 MiB\n",
      "  Total used:        444 MiB (0.434 GiB)\n",
      "\n",
      "Model Info:\n",
      "  Total Parameters: 8,012,548\n",
      "  Output Shape: torch.Size([2, 100])\n",
      "  After cleanup:     210 MiB\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Model: R3D_18\n",
      "================================================================================\n",
      "\n",
      "Inference Time: 88.19 ms\n",
      "Throughput: 22.68 samples/sec\n",
      "\n",
      "GPU Memory Usage (via nvidia-smi):\n",
      "  Before model load: 210 MiB\n",
      "  After model load:  348 MiB  (+138 MiB)\n",
      "  Before inference:  848 MiB\n",
      "  Peak during run:   946 MiB\n",
      "  Total used:        946 MiB (0.924 GiB)\n",
      "\n",
      "Model Info:\n",
      "  Total Parameters: 33,217,572\n",
      "  Output Shape: torch.Size([2, 100])\n",
      "  After cleanup:     314 MiB\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Model: R(2+1)D_18\n",
      "================================================================================\n",
      "\n",
      "Inference Time: 127.22 ms\n",
      "Throughput: 15.72 samples/sec\n",
      "\n",
      "GPU Memory Usage (via nvidia-smi):\n",
      "  Before model load: 314 MiB\n",
      "  After model load:  360 MiB  (+46 MiB)\n",
      "  Before inference:  1362 MiB\n",
      "  Peak during run:   1362 MiB\n",
      "  Total used:        1362 MiB (1.330 GiB)\n",
      "\n",
      "Model Info:\n",
      "  Total Parameters: 31,351,425\n",
      "  Output Shape: torch.Size([2, 100])\n",
      "  After cleanup:     316 MiB\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Model: Swin3D_T\n",
      "================================================================================\n",
      "\n",
      "Inference Time: 86.10 ms\n",
      "Throughput: 23.23 samples/sec\n",
      "\n",
      "GPU Memory Usage (via nvidia-smi):\n",
      "  Before model load: 316 MiB\n",
      "  After model load:  362 MiB  (+46 MiB)\n",
      "  Before inference:  1218 MiB\n",
      "  Peak during run:   1218 MiB\n",
      "  Total used:        1218 MiB (1.189 GiB)\n",
      "\n",
      "Model Info:\n",
      "  Total Parameters: 27,927,370\n",
      "  Output Shape: torch.Size([2, 100])\n",
      "  After cleanup:     316 MiB\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Model: Swin3D_S\n",
      "================================================================================\n",
      "\n",
      "Inference Time: 190.57 ms\n",
      "Throughput: 10.49 samples/sec\n",
      "\n",
      "GPU Memory Usage (via nvidia-smi):\n",
      "  Before model load: 316 MiB\n",
      "  After model load:  592 MiB  (+276 MiB)\n",
      "  Before inference:  1748 MiB\n",
      "  Peak during run:   1748 MiB\n",
      "  Total used:        1748 MiB (1.707 GiB)\n",
      "\n",
      "Model Info:\n",
      "  Total Parameters: 87,741,484\n",
      "  Output Shape: torch.Size([2, 100])\n",
      "  After cleanup:     316 MiB\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Model: Swin3D_B\n",
      "================================================================================\n",
      "\n",
      "Inference Time: 190.73 ms\n",
      "Throughput: 10.49 samples/sec\n",
      "\n",
      "GPU Memory Usage (via nvidia-smi):\n",
      "  Before model load: 316 MiB\n",
      "  After model load:  592 MiB  (+276 MiB)\n",
      "  Before inference:  1748 MiB\n",
      "  Peak during run:   1748 MiB\n",
      "  Total used:        1748 MiB (1.707 GiB)\n",
      "\n",
      "Model Info:\n",
      "  Total Parameters: 87,741,484\n",
      "  Output Shape: torch.Size([2, 100])\n",
      "  After cleanup:     316 MiB\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Model: MViTv2_S\n",
      "================================================================================\n",
      "\n",
      "Inference Time: 119.41 ms\n",
      "Throughput: 16.75 samples/sec\n",
      "\n",
      "GPU Memory Usage (via nvidia-smi):\n",
      "  Before model load: 316 MiB\n",
      "  After model load:  366 MiB  (+50 MiB)\n",
      "  Before inference:  1302 MiB\n",
      "  Peak during run:   1302 MiB\n",
      "  Total used:        1302 MiB (1.271 GiB)\n",
      "\n",
      "Model Info:\n",
      "  Total Parameters: 34,307,044\n",
      "  Output Shape: torch.Size([2, 100])\n",
      "  After cleanup:     316 MiB\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Model: MViTv1_B\n",
      "================================================================================\n",
      "\n",
      "Inference Time: 117.39 ms\n",
      "Throughput: 17.04 samples/sec\n",
      "\n",
      "GPU Memory Usage (via nvidia-smi):\n",
      "  Before model load: 316 MiB\n",
      "  After model load:  366 MiB  (+50 MiB)\n",
      "  Before inference:  1302 MiB\n",
      "  Peak during run:   1302 MiB\n",
      "  Total used:        1302 MiB (1.271 GiB)\n",
      "\n",
      "Model Info:\n",
      "  Total Parameters: 34,307,044\n",
      "  Output Shape: torch.Size([2, 100])\n",
      "  After cleanup:     316 MiB\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Re-initializing models...\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/miniconda3/envs/wlasl/lib/python3.10/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Replace cell 12 with this version that monitors memory DURING inference\n",
    "\n",
    "import time\n",
    "import subprocess\n",
    "import threading\n",
    "\n",
    "def get_gpu_memory():\n",
    "    \"\"\"Get actual GPU memory usage in MiB using nvidia-smi\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader'],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=True\n",
    "        )\n",
    "        return int(result.stdout.strip())\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "class MemoryMonitor:\n",
    "    \"\"\"Monitor GPU memory in a background thread during inference\"\"\"\n",
    "    def __init__(self, interval=0.001):  # Sample every 1ms\n",
    "        self.interval = interval\n",
    "        self.monitoring = False\n",
    "        self.peak_memory = 0\n",
    "        self.memory_samples = []\n",
    "        self.thread = None\n",
    "    \n",
    "    def _monitor(self):\n",
    "        while self.monitoring:\n",
    "            mem = get_gpu_memory()\n",
    "            if mem is not None:\n",
    "                self.memory_samples.append(mem)\n",
    "                self.peak_memory = max(self.peak_memory, mem)\n",
    "            time.sleep(self.interval)\n",
    "    \n",
    "    def start(self):\n",
    "        self.monitoring = True\n",
    "        self.peak_memory = 0\n",
    "        self.memory_samples = []\n",
    "        self.thread = threading.Thread(target=self._monitor, daemon=True)\n",
    "        self.thread.start()\n",
    "    \n",
    "    def stop(self):\n",
    "        self.monitoring = False\n",
    "        if self.thread:\n",
    "            self.thread.join()\n",
    "        return self.peak_memory, self.memory_samples\n",
    "\n",
    "# Clear GPU memory before starting\n",
    "torch.cuda.empty_cache()\n",
    "time.sleep(0.5)\n",
    "\n",
    "baseline_memory = get_gpu_memory()\n",
    "print(f\"Baseline GPU Memory: {baseline_memory} MiB ({baseline_memory/1024:.3f} GiB)\\n\")\n",
    "\n",
    "for arch, model in all_models:\n",
    "    # Clear everything first\n",
    "    torch.cuda.empty_cache()\n",
    "    time.sleep(0.3)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Model: {arch}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    mem_before = get_gpu_memory()\n",
    "    \n",
    "    # Load model to GPU\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    torch.cuda.synchronize()\n",
    "    time.sleep(0.2)\n",
    "    \n",
    "    mem_after_model = get_gpu_memory()\n",
    "    \n",
    "    # Prepare input\n",
    "    vid0_gpu = vid0.to(device)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Warm-up run\n",
    "    with torch.no_grad():\n",
    "        _ = model(vid0_gpu)\n",
    "    torch.cuda.synchronize()\n",
    "    time.sleep(0.2)\n",
    "    \n",
    "    mem_after_warmup = get_gpu_memory()\n",
    "    \n",
    "    # Create memory monitor\n",
    "    monitor = MemoryMonitor(interval=0.001)  # Sample every 1ms\n",
    "    \n",
    "    # Start monitoring and run inference\n",
    "    monitor.start()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        output = model(vid0_gpu)\n",
    "    torch.cuda.synchronize()\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Stop monitoring\n",
    "    peak_memory, samples = monitor.stop()\n",
    "    \n",
    "    # Model parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    print(f\"\\nInference Time: {(end_time - start_time)*1000:.2f} ms\")\n",
    "    print(f\"Throughput: {bs / (end_time - start_time):.2f} samples/sec\")\n",
    "    print(f\"Memory samples taken: {len(samples)}\")\n",
    "    \n",
    "    print(f\"\\nGPU Memory Usage (via nvidia-smi):\")\n",
    "    print(f\"  Baseline:          {baseline_memory} MiB ({baseline_memory/1024:.3f} GiB)\")\n",
    "    print(f\"  Before model:      {mem_before} MiB\")\n",
    "    print(f\"  After model load:  {mem_after_model} MiB  (+{mem_after_model - mem_before} MiB)\")\n",
    "    print(f\"  After warmup:      {mem_after_warmup} MiB\")\n",
    "    print(f\"  PEAK during run:   {peak_memory} MiB ({peak_memory/1024:.3f} GiB)\")\n",
    "    print(f\"  Peak over baseline: +{peak_memory - baseline_memory} MiB\")\n",
    "    \n",
    "    if len(samples) > 0:\n",
    "        avg_memory = sum(samples) / len(samples)\n",
    "        print(f\"  Average during run: {int(avg_memory)} MiB ({avg_memory/1024:.3f} GiB)\")\n",
    "    \n",
    "    print(f\"\\nModel Info:\")\n",
    "    print(f\"  Total Parameters: {total_params:,}\")\n",
    "    print(f\"  Output Shape: {output.shape}\")\n",
    "    \n",
    "    # Clean up for next model\n",
    "    del output\n",
    "    model.cpu()\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    time.sleep(0.3)\n",
    "    \n",
    "    mem_after_cleanup = get_gpu_memory()\n",
    "    print(f\"  After cleanup:     {mem_after_cleanup} MiB ({mem_after_cleanup/1024:.3f} GiB)\")\n",
    "    \n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Recreate model list\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Re-initializing models...\")\n",
    "print(\"=\"*80)\n",
    "all_models = []\n",
    "for arch in avail_m:\n",
    "    model = get_model(arch, nc, dropout)\n",
    "    all_models.append((arch, model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wlasl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
