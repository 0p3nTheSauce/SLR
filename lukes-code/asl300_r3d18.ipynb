{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "336aae8c",
   "metadata": {},
   "source": [
    "# Going to try train with more classes as the model was overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da0600c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 02:50:02.930283: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-20 02:50:02.939423: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-20 02:50:02.949610: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-20 02:50:02.952745: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-20 02:50:02.960596: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-20 02:50:03.416875: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision.models.video as video_models\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import json\n",
    "\n",
    "from train import train_model_4\n",
    "import video_dataset as Dataset\n",
    "# from test import test_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a40ca7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 3549\n",
      "Number of training classes: 300\n",
      "Number of validation samples: 901\n",
      "Number of validation classes: 300\n"
     ]
    }
   ],
   "source": [
    "train_inst_path = './preprocessed/labels/asl300/train_instances_fixed_frange_bboxes_len.json'\n",
    "train_clss_path = './preprocessed/labels/asl300/train_classes_fixed_frange_bboxes_len.json'\n",
    "val_inst_path = './preprocessed/labels/asl300/val_instances_fixed_frange_bboxes_len.json'\n",
    "val_clss_path = './preprocessed/labels/asl300/val_classes_fixed_frange_bboxes_len.json'\n",
    "raw_path = '../data/WLASL2000'\n",
    "transform0 = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: Dataset.correct_num_frames(x, 16)),  # (T, C, H, W)\n",
    "    transforms.Lambda(lambda x: x.float() / 255.0),  # Convert to float and normalize to [0,1]\n",
    "    transforms.Lambda(lambda x: F.interpolate(x, size=(112, 112), mode='bilinear', align_corners=False)),  # Resize after normalization\n",
    "    transforms.Lambda(lambda x: Dataset.normalise(x,  mean=[0.43216, 0.394666, 0.37645], std=[0.22803, 0.22145, 0.216989])),  # Normalize per channel\n",
    "    transforms.Lambda(lambda x: x.permute(1, 0, 2, 3)),  # (T, C, H, W) -> (C, T, H, W)\n",
    "]) #The transform that got the best result\n",
    "train_set = Dataset.VideoDataset(\n",
    "    root=raw_path,\n",
    "    instances_path=train_inst_path,\n",
    "    classes_path=train_clss_path,\n",
    "    transform=transform0\n",
    ")\n",
    "val_set = Dataset.VideoDataset(\n",
    "    root=raw_path,\n",
    "    instances_path=val_inst_path,\n",
    "    classes_path=val_clss_path,\n",
    "    transform=transform0\n",
    ")\n",
    "print(f\"Number of training samples: {len(train_set)}\")\n",
    "print(f\"Number of training classes: {len(set(train_set.classes))}\")\n",
    "print(f\"Number of validation samples: {len(val_set)}\")\n",
    "print(f\"Number of validation classes: {len(set(val_set.classes))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a5e5b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7a12d8101cc0>\n",
      "Validation loader:\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7a12d8100b20>\n"
     ]
    }
   ],
   "source": [
    "# torch.manual_seed(42) #probably doesnt work because of numworkers\n",
    "train_loader = DataLoader(\n",
    "  train_set,\n",
    "  batch_size=32, \n",
    "  shuffle=True,\n",
    "  num_workers=2, #this was 4 but I previously had issues with the computer crashing (though this was with more data)\n",
    "  drop_last=True\n",
    ")\n",
    "\n",
    "print(f'Train loader:\\n{train_loader}')\n",
    "\n",
    "val_loader = DataLoader(\n",
    "  val_set,\n",
    "  batch_size=32,\n",
    "  shuffle=False,\n",
    "  drop_last=False,\n",
    "  num_workers=2\n",
    ")\n",
    "\n",
    "print(f\"Validation loader:\\n{val_loader}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "803f94c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f80d0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/miniconda3/envs/wlasl/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/luke/miniconda3/envs/wlasl/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=R3D_18_Weights.KINETICS400_V1`. You can also use `weights=R3D_18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training parameter: stem.0.weight\n",
      "Training parameter: stem.1.weight\n",
      "Training parameter: stem.1.bias\n",
      "Training parameter: layer1.0.conv1.0.weight\n",
      "Training parameter: layer1.0.conv1.1.weight\n",
      "Training parameter: layer1.0.conv1.1.bias\n",
      "Training parameter: layer1.0.conv2.0.weight\n",
      "Training parameter: layer1.0.conv2.1.weight\n",
      "Training parameter: layer1.0.conv2.1.bias\n",
      "Training parameter: layer1.1.conv1.0.weight\n",
      "Training parameter: layer1.1.conv1.1.weight\n",
      "Training parameter: layer1.1.conv1.1.bias\n",
      "Training parameter: layer1.1.conv2.0.weight\n",
      "Training parameter: layer1.1.conv2.1.weight\n",
      "Training parameter: layer1.1.conv2.1.bias\n",
      "Training parameter: layer2.0.conv1.0.weight\n",
      "Training parameter: layer2.0.conv1.1.weight\n",
      "Training parameter: layer2.0.conv1.1.bias\n",
      "Training parameter: layer2.0.conv2.0.weight\n",
      "Training parameter: layer2.0.conv2.1.weight\n",
      "Training parameter: layer2.0.conv2.1.bias\n",
      "Training parameter: layer2.0.downsample.0.weight\n",
      "Training parameter: layer2.0.downsample.1.weight\n",
      "Training parameter: layer2.0.downsample.1.bias\n",
      "Training parameter: layer2.1.conv1.0.weight\n",
      "Training parameter: layer2.1.conv1.1.weight\n",
      "Training parameter: layer2.1.conv1.1.bias\n",
      "Training parameter: layer2.1.conv2.0.weight\n",
      "Training parameter: layer2.1.conv2.1.weight\n",
      "Training parameter: layer2.1.conv2.1.bias\n",
      "Training parameter: layer3.0.conv1.0.weight\n",
      "Training parameter: layer3.0.conv1.1.weight\n",
      "Training parameter: layer3.0.conv1.1.bias\n",
      "Training parameter: layer3.0.conv2.0.weight\n",
      "Training parameter: layer3.0.conv2.1.weight\n",
      "Training parameter: layer3.0.conv2.1.bias\n",
      "Training parameter: layer3.0.downsample.0.weight\n",
      "Training parameter: layer3.0.downsample.1.weight\n",
      "Training parameter: layer3.0.downsample.1.bias\n",
      "Training parameter: layer3.1.conv1.0.weight\n",
      "Training parameter: layer3.1.conv1.1.weight\n",
      "Training parameter: layer3.1.conv1.1.bias\n",
      "Training parameter: layer3.1.conv2.0.weight\n",
      "Training parameter: layer3.1.conv2.1.weight\n",
      "Training parameter: layer3.1.conv2.1.bias\n",
      "Training parameter: layer4.0.conv1.0.weight\n",
      "Training parameter: layer4.0.conv1.1.weight\n",
      "Training parameter: layer4.0.conv1.1.bias\n",
      "Training parameter: layer4.0.conv2.0.weight\n",
      "Training parameter: layer4.0.conv2.1.weight\n",
      "Training parameter: layer4.0.conv2.1.bias\n",
      "Training parameter: layer4.0.downsample.0.weight\n",
      "Training parameter: layer4.0.downsample.1.weight\n",
      "Training parameter: layer4.0.downsample.1.bias\n",
      "Training parameter: layer4.1.conv1.0.weight\n",
      "Training parameter: layer4.1.conv1.1.weight\n",
      "Training parameter: layer4.1.conv1.1.bias\n",
      "Training parameter: layer4.1.conv2.0.weight\n",
      "Training parameter: layer4.1.conv2.1.weight\n",
      "Training parameter: layer4.1.conv2.1.bias\n",
      "Training parameter: fc.weight\n",
      "Training parameter: fc.bias\n",
      "Set stem.1 to eval mode (frozen layer)\n",
      "Set layer1.0.conv1.1 to eval mode (frozen layer)\n",
      "Set layer1.0.conv2.1 to eval mode (frozen layer)\n",
      "Set layer1.1.conv1.1 to eval mode (frozen layer)\n",
      "Set layer1.1.conv2.1 to eval mode (frozen layer)\n",
      "Set layer2.0.conv1.1 to eval mode (frozen layer)\n",
      "Set layer2.0.conv2.1 to eval mode (frozen layer)\n",
      "Set layer2.0.downsample.1 to eval mode (frozen layer)\n",
      "Set layer2.1.conv1.1 to eval mode (frozen layer)\n",
      "Set layer2.1.conv2.1 to eval mode (frozen layer)\n",
      "Set layer3.0.conv1.1 to eval mode (frozen layer)\n",
      "Set layer3.0.conv2.1 to eval mode (frozen layer)\n",
      "Set layer3.0.downsample.1 to eval mode (frozen layer)\n",
      "Set layer3.1.conv1.1 to eval mode (frozen layer)\n",
      "Set layer3.1.conv2.1 to eval mode (frozen layer)\n"
     ]
    }
   ],
   "source": [
    "model = video_models.r3d_18(pretrained=True)\n",
    "num_classes = 300\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "for param in model.parameters():\n",
    "  param.requires_grad = True\n",
    "  \n",
    "for layer_name in ['layer4', 'fc']:\n",
    "  if hasattr(model, layer_name):\n",
    "    for param in getattr(model, layer_name).parameters():\n",
    "      param.requires_grad = True\n",
    "      \n",
    "for name, param in model.named_parameters():\n",
    "  if param.requires_grad:\n",
    "    print(f\"Training parameter: {name}\")\n",
    "  else:\n",
    "    print(f\"Freezing parameter: {name}\")\n",
    "    \n",
    "for name, module in model.named_modules():\n",
    "  if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):\n",
    "    # Check if this BatchNorm is in a frozen layer\n",
    "    is_in_frozen_layer = not any(unfreeze_layer in name for unfreeze_layer in ['layer4', 'fc'])\n",
    "    \n",
    "    if is_in_frozen_layer:\n",
    "      module.eval()\n",
    "      module.track_running_stats = False\n",
    "      print(f\"Set {name} to eval mode (frozen layer)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2b32fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(trainable_params, lr=1e-4)  #this learning rate might be too high\n",
    "#TODO : try this code\n",
    "# optimizer = torch.optim.Adam([\n",
    "#     {'params': model.layer4.parameters(), 'lr': 1e-4},\n",
    "#     {'params': model.fc.parameters(), 'lr': 1e-3}  # Higher LR for new classifier\n",
    "# ])\n",
    "print(len(trainable_params), \"trainable parameters\")\n",
    "loss_func = nn.CrossEntropyLoss() #TODO : try Contrastive loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa745000",
   "metadata": {},
   "outputs": [],
   "source": [
    "schedular = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "  optimizer,\n",
    "  mode='min',\n",
    "  factor=0.1,\n",
    "  patience=15,\n",
    ") #not sure if i should use the schedular, but will try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb479ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory set to: runs/asl300/r3d18_exp0\n",
      "Save directory set to: runs/asl300/r3d18_exp0/checkpoints\n",
      "Logs directory set to: runs/asl300/r3d18_exp0/logs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training R3D:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1/100:\n",
      "  Train Loss: 5.6973, Train Acc: 1.70%\n",
      "  Val Loss: 5.5110, Val Acc: 1.78%\n",
      "  Learning Rate: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training R3D:   1%|          | 1/100 [02:08<3:31:12, 128.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 2/100:\n",
      "  Train Loss: 4.7047, Train Acc: 19.23%\n",
      "  Val Loss: 5.0608, Val Acc: 9.10%\n",
      "  Learning Rate: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training R3D:   2%|▏         | 2/100 [04:16<3:29:57, 128.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 3/100:\n",
      "  Train Loss: 3.6868, Train Acc: 55.11%\n",
      "  Val Loss: 4.7698, Val Acc: 12.87%\n",
      "  Learning Rate: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training R3D:   3%|▎         | 3/100 [07:45<4:10:55, 155.21s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_4\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m  \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m  \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m  \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m  \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m  \u001b[49m\u001b[43mschedular\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschedular\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m  \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mruns/asl300/r3d18_exp0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ExtraStorage/WLASL/lukes-code/train.py:186\u001b[0m, in \u001b[0;36mtrain_model_4\u001b[0;34m(model, train_loader, optimizer, loss_func, epochs, val_loader, schedular, output, logs, save, save_every, load)\u001b[0m\n\u001b[1;32m    183\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m#Accumulate metrics\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m data\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m#weight by batch size\u001b[39;00m\n\u001b[1;32m    187\u001b[0m train_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    188\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m model_output\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = train_model_4(\n",
    "  model=model,\n",
    "  train_loader=train_loader,\n",
    "  optimizer=optimizer,\n",
    "  loss_func=loss_func,\n",
    "  epochs=100,\n",
    "  val_loader=val_loader,\n",
    "  schedular=schedular,\n",
    "  output='runs/asl300/r3d18_exp0'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wlasl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
